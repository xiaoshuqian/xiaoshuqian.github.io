<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>
            Numerical Linear Algebra
        </title>
    </head>
    <body>
        <h1>
            Numerical Linear Algebra
        </h1>
        <h2>
            Quarter offered
        </h2>
        <p>
        
        </p>
        <h2>
            Course description
        </h2>
        <h4>
            Instructor
        </h4>
        <p>
        Dr. Xiaoshu Qian
        </p>
        <h4>
            Reference textbook(s)
        </h4>
        <ul>
            <li>Book by Trefethen and Bau
              <ul>
                <li><a href="http://people.maths.ox.ac.uk/~trefethen/text.html">NUMERICAL LINEAR ALGEBRA</a>
                </li>
                <li><a href="http://www.ams.sunysb.edu/~jiao/teaching/ams526_fall16/">Course at Stony-Brook based on the book</a>, <a href="http://www.ams.sunysb.edu/~jiao/teaching/ams526_fall16/schedule.html">Slides</a>
                  (old: <a href="http://www.ams.sunysb.edu/~jiao/teaching/ams526_fall15/#Cprog">Fall 2015</a>, <a href="http://www.ams.sunysb.edu/~jiao/teaching/ams526_fall15/schedule.html">Slides</a>)
                </li>
              </ul>
            </li>
            <li><a href="https://www.mathworks.com/moler/chapters.html">Book and matlab scripts by Steve Moler</a>
            </li>
        </ul>
        <h4>
            Description
        </h4>
        <p>
        From ITU course catalog:
        </p>
        <h4>
            Detailed outline
        </h4>
<ul>
  <li>LU decomposition and Cholesky decomposition
  </li>
  <li>Solve Ax = b using QR decomposition
    <ul>
      <li>Compute QRD: A = QR
      </li>
      <li>Compute Q<sup>*</sup>b by left multiplying b by Q<sup>*</sup>
      </li>
      <li>Solve Rx = Q<sup>*</sup>b by back-substitution
      </li>
    </ul>
  </li>
  <li><a href="https://blogs.mathworks.com/cleve/2016/07/25/compare-gram-schmidt-and-householder-orthogonalization-algorithms/">QR decomposition using GS, MGS, Householder</a> and Givens
    <ul>
      <li>All four algorithms compute Q and R that do a good job of reproducing the data A (i.e. norm(A-Q*R,inf)/norm(A, inf) is small).
      </li>
      <li>Their behavior is very different when it comes to producing orthogonality (i.e. norm(Q'*Q - eye(n), inf) = ?).
        <ul>
          <li>Classic Gram-Schmidt. Usually very poor orthogonality.
          </li>
          <li>Modified Gram-Schmidt. Depends upon condition of A. Fails completely when X is singular.
          </li>
          <li>Householder and Givens. Always good orthogonality.
          </li>
        </ul>
      </li>
      <li><a href=https://books.google.com/books?id=-tW8-FUoxWwC&pg=PA207&lpg=PA207&dq=Householder+transformation+vs+MGS+table&source=bl&ots=9XXZmQmQBb&sig=ACfU3U1lDSDc31we1BRPq7N0XOR0pm3Ncw&hl=en&ppis=_e&sa=X&ved=2ahUKEwjPlLLA4aLoAhUYpp4KHZuPDa4Q6AEwAHoECAkQAQ#v=onepage&q=Householder%20transformation%20vs%20MGS%20table&f=false>Complexity</a> (<a href=https://my.siam.org/Store/Product/viewproduct/?ProductId=1016>Numerical Linear Algebra and Applications, 2nd Edition</a>):
        <ul>
          <li>CGS and MGS: 2mn<sup>2</sup>
          </li>
          <li>HT: 2(m-n/3)n<sup>2</sup>
          </li>
          <li>Givens: 3(m-n/3)n<sup>2</sup>
          </li>
        </ul>
      </li>
      <li>MGS: The idea is to orthogonalize against the emerging set of vectors instead of against the original set. There are two variants, a column-oriented one and a row-oriented one, that produce the same results.
        <ul>
          <li>Row version of the MGS can be viewed as triangular orthogonalization by right multiplication of a sequence of right triangular matrices (see Trefethen and Bau or this <a href="https://dspace.mit.edu/bitstream/handle/1721.1/75282/18-335j-fall-2006/contents/lecture-notes/lec6handout6pp.pdf">slide</a>). Cf: LU decompositon can be viewed as producing an upper trianular matrix by left multiplication of a sequence of left triangular matrices.
          </li>
          <li>
            <a href="https://www.quora.com/Why-is-modified-Gram-Schmidt-more-numerically-stable-than-classical-Gram-Schmidt">Why is MGS more stable?</a> Each time you orthogonalize you introduce some small error in random directions. With classical GS, these errors all add up; with modified GS they get eliminated.

Compare: if you compute the subtraction of q0 and q1 separately from the k-th vector, they all introduce random error.

On the other hand, if you first subtract q0 (times the right coefficient), it introduces error in direction of q1, but that error is then eliminated when you compute the coefficient for subtracting q1. Et cetera.
          </li>
        </ul>
      </li>
      <li><a href="https://dspace.mit.edu/bitstream/handle/1721.1/75282/18-335j-fall-2006/contents/lecture-notes/lec6handout6pp.pdf">Householder</a>: A Householder reflection H transforms a given vector x into a multiple of a unit vector e1 while preserving length, so Hx=&mnplus;s*e1 where s=||x||. The matrix H is never formed. The reflection is expressed as a rank-one modification of the identity: H=I-uu* where ||u||=sqrt(2). Reflectors can be used to compute
        <ul>
          <li>Q<sup>*</sup>x
          </li>
          <li>Qx
          </li>
          <li>Q
          </li>
        </ul>
      </li>
      <li>Householder for solving LS problem y = Hx: Append y to the columns in H to form matrix [H, y] before applying the QR to obtain an m by (m+1) R matrix. The resulting R triangular matrix equation z = R'x can be obtained by extracting R' and z from the first m columns and last column in R respectively. This removes the need to save the Q matrix.
      </li>
      <li>Householder for solving MMSE or regularized LS problem y = Hx: Append a diagonal matrix whose diagonal elements are noise std dev (assumming uncorrelated noise) at the bottom of H and then apply QR. Note at each of n steps of the QR procedure, we need only to apply HT on a column vector of size m since the 0's below the diagonal element of the appended diagonal matrix can be ignored while applying HT.
      </li>
    </ul>
  </li>
  <li>Iterative methods for linear systems
    <ul>
      <li>Jacobi
      </li>
      <li>Gauss-Seidel
      </li>
      <li>SOR: Successive-Over-relaxation
      </li>
      <li>Conjugate gradient for positive definite symmetric matrix, MINRES for SPD matrix
      </li>
      <li>GMRES, CGN for non-symmetric matrix
      </li>
    </ul>
  </li>
  <li>Iterative methods for Eigen-value system
    <ul>
      <li>Power and inverse power with shift
      </li>
      <li>QR with 1) shift 2)get more zeros in off-diagonal positions (make it Hessenberg)
      </li>
      <li>Lanczos algorithm for Hermitian matrix
      </li>
      <li>Arnoldi iteration for non-hermitian matrix
      </li>
    </ul>
  </li>
  <li><br>
  <table>
    <ul>
                    <caption>Iterative Methods</caption>
                    <tr>
                        <th></th>
                        <th>Linear Eq</th>
                        <th>Eigen-values</th>
                    </tr>
                    <tr>
                        <td>Hermitian</td>
                        <td>Conjugate Grad<br>
                          MINRES (SPD matrix)
                        </td>
                        <td>Lanczos</td>
                    </tr>
                    <tr>
                        <td>Non-hermitian</td>
                        <td>GMRES<br>
                          CGN
                        </td>
                        <td>Arnoldi</td>
                    </tr>
                </table>
    </ul>
  </li>
</ul>
    </body>
</html>
