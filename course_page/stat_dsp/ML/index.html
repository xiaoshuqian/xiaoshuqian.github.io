<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Machine Learning</title>
    </head>
    <body>
        <h1>CSC 633-1 Machine Learning</h1>
        <h2>
            Trimester offered
        </h2>
        <ul>
          <li>Fall 2019, online
          </li>
          <li>Summer 2019, Noon -- 8pm, 5/11,5/12, 6/15,6/16, 7/20, 7/21; Room 303.
          </li>
        </ul>
        <h2>
          Prerequisites
        </h2>
        <p>
        Recommended: Knowledge of basic computer science principles and skills, probability theory, and linear algebra.
        </p>
        <h2>
            Course description
        </h2>
        <h4>
            Instructor
        </h4>
        <p>
        Dr. Xiaoshu Qian
        </p>
        <h4>
            References
        </h4>
        <ul>
          <li>Books:
            <ul>
              <li><a href="http://people.sabanciuniv.edu/berrin/cs512/lectures/Book-Mitchell-onlinebook.pdf">Machine Learning</a> by Tom Mitchell, <a href="http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html">chapter slides</a>
              </li>
              <li>Understanding Machine Learning: From Theory to Algorithms, <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html">book_site (with pdf download)</a>
              </li>
              <li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Elements of Statistical Learning</a> by Trevor Hastie, Robert Tibshirani, Jerome Friedman
              </li>
              <li><a href="http://www.deeplearningbook.org/">Deep Learning</a> by Ian Goodfellow and Yoshua Bengio and Aaron Courville, MIT press, with links to slides, exercises and videos
              </li>
            </ul>
          </li>
          <li>Courses:
            <ul>
              <li><a href="https://www.coursera.org/specializations/aml">Advanced ML specialization</a> 7 coureses, offered by National Research University Higher School of Economics in Russia
              </li>
              <li><a href="https://www.coursera.org/specializations/machine-learning">ML specialization</a> 4 coureses, by Carlos Guestrin (Creater of GraphLab) and Emily Fox
              </li>
              <li><a href="https://www.coursera.org/learn/neural-networks">NN for ML</a> 16 weeks coureses, by Geoffrey Hinton from U of Toronto. Course has ended. Quiz links do not work any more. But slides are good
              </li>
              <li><a href="https://www.coursera.org/learn/introduction-tensorflow">TensorFlow for ML</a>
              </li>
              <li><a href="https://www.coursera.org/learn/machine-learning">ML_Andrew_Ng (intro)</a>
              </li>
              <li><a href="http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml">ML course at Carnegie Mellon</a> with slides, lecture video and HW
                <ul>
                  <li><a href="http://www.cs.cmu.edu/~10601b/slides/">slides</a> from another <a href="http://www.cs.cmu.edu/~10601b/">course</a>
                  </li>
                </ul>
              </li>
              <li><a href="https://people.eecs.berkeley.edu/~jrs/189/">ML course at Berkerley</a> with slides, HW, many exams
              </li>
            </ul>
          </li>
          <li>Other courses:
            <ul>
              <li><a href="https://course.fast.ai/">Practical Deep Learning for Coders</a>
              </li>
              <li><a href="https://www.udemy.com/user/lazy-programmer/">Lazy Programmer Inc</a> 25 courese, ~$20 per video course, not sure if slides are available
              </li>
              <li>From Udacity:
                <ul>
                  <li><a href="https://developers.google.com/machine-learning/crash-course/">ML crash course</a> offered by Google for free
                  </li>
                  <li><a href="https://www.udacity.com/course/deep-learning--ud730">deep learning (ud730)</a> offered by Google for free
                  </li>
                  <li><a href="https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101">deep learning (ud101)</a>There's a waiting list but I'm not sure it's free.
                  </li>
                  <li><a href="https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009">ML engineer (nd009)</a>
                    Those with "nanodegree" label seem to be part of a graduate where you pay 199$/month (Basic) or 299$/month (Plus). The plus version guarantees getting a job or 100% refund.
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Others:
            <ul>
              <li>Tutorials: <a href="https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/">try_your_own_images</a>, <a href="https://www.datacamp.com/community/tutorials/deep-learning-python">datacamp</a>, <a href="https://www.pyimagesearch.com/2014/05/26/opencv-python-k-means-color-clustering/">k-mean using scikit-learn</a>
              </li>
              <li><a href="https://explore.mathworks.com/machine-learning-knowledge-quiz/landing-8ZT-16C.html">ML_quiz_example</a>
              </li>
              <li><a href="https://colab.research.google.com/notebooks/welcome.ipynb">colab</a>
              </li>
            </ul>
          </li>
        </ul>
        <h4>
            Description
        </h4>
        <p> From ITU course catalog:
Machine learning is a fast-moving field with many recent real world commercial applications. The goal of Machine Learning is to build computer model that can produce useful information whether predictions, associations, or classifications. The ultimate goal for many machine learning researchers is to build computing systems that can automatically adapt and learn from their experience. This course will study the theory and practical algorithms in Machine Learning. It reviews what machine learning is about, how it evolved over the past 60 years, why it is important today, basic concepts and paradigms, what key techniques, challenges and tricks. It also cover examples of how machine learning is used/applied today in the real world, and expose students to some experience in building and using machine learning algorithms. This course will also discuss recent applications of machine learning, such as to robotic control, speech recognition, face recognition, data mining, autonomous navigation, bioinformatics, and text and web data processing.
        </p>
        <h4> Detailed outline </h4>
        <ul>
            <li> Overview
                <ul>
                    <li><a href="https://medium.com/datadriveninvestor/differences-between-ai-and-machine-learning-and-why-it-matters-1255b182fc6">AI vs ML</a>, <a href="https://www.newgenapps.com/blog/artificial-intelligence-vs-machine-learning-vs-data-science">AI vs ML vs data science</a>
                        <ul>
                            <li><a href="https://medium.com/app-affairs/9-applications-of-machine-learning-from-day-to-day-life-112a47a429d0">ML Applications from day-to-day life</a>
                            </li>
                        </ul>
                    </li>
                    <li>ML techniques
                        <ul>
                            <li>Supervised:
                              <ul>
                                <li>Regression/Estimation: k-NN, Naive-Bayes, SVM or SVR (SV Regression), Linear Regression, Neuron Network
                                </li>
                                <li>Classification/Detection: k-NN, Naive-Bayes, Decision Tree, SVM, Logistic Regression, Neuron Network
                                </li>
                              </ul>
                            </li>
                            <li>Unsupervised: k-mean, soft k-mean
                            </li>
                            <li>Reinforcement</li>
                        </ul>
                    </li>
                    <li>Traditional programming paradigm vs ML programming paradigm
                        <ul>
                            <li>Traditional: data + rule -> answer: imperative, functional, OO, declarative</li>
                            <li>ML: data + answer -> rule</li>
                            <li>Releated: <a href="https://www.smartsheet.com/knowledge-base-systems-and-templates">Knowledge-based system</a>, logic programming</li>
                        </ul>
                    </li>
                    <li>Bias vs variance
                    </li>
                    <li>Training, testing, validation
                    </li>
                </ul>
            </li>
            <li><a href="https://towardsdatascience.com/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11">Popular algorithms in ML</a> (see also <a href="https://towardsdatascience.com/top-10-machine-learning-algorithms-for-data-science-cdb0400a25f9">here</a>)
                <ul>
                    <li>Linear regression: model architecture, loss function, optimizer (sgd & learning rates), scaling data, training, testing, validation, prediction, regularization
                    </li>
                    <li>Logistic regression
                    </li>
                    <li>Neural network (Feed-forward, Convolutional, Reccurrent, learning vector quantization, etc)
                    </li>
                    <li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear desriminant analysis</a> (cf ANOVA, PCA, regression, etc)
                    </li>
                    <li>PCA (cf <a href="https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi">FA</a>)
                    </li>
                    <li>Naive Bayes: <a href="https://www.youtube.com/watch?v=7yB26ObUT6Y">NB using sklearn</a>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">Gaussian</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html">Bernoulli</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">Multinomial</a>, <a href="https://scikit-learn.org/stable/modules/classes.html">API ref</a>
                    </li>
                    <li>K-Nearest neighbor: <a href="https://www.youtube.com/watch?v=HVXime0nQeI">A primer</a> and <a href="https://www.youtube.com/watch?v=4HKqjENq9OU">K-NN using sklearn</a>
                      <ul>
                        <li>KNN can be used for both classification and regression
                        </li>
                        <li>Idea: Find K nearest neighbors and use the majority/median of its neighbors' labels as the predictor
                        </li>
                        <li>Algorithm:
                          <ul>
                            <li>Select K based on tests on a subset of data (typically a too small K causes sensitivity to noise and over-fitting while a too large K is prone to bias towards more probable labels)
                            </li>
                            <li>Select distance metric
                            </li>
                            <li>Find the distances of the unknown case from all other cases
                            </li>
                            <li>Find K nearest neighbors of the unknown case
                            </li>
                            <li>Use the majority/median of its neighbors' labels as the predictor for the unknown case
                            </li>
                          </ul>
                        </li>
                        <li>Pros
                          <ul>
                            <li>Simple: almost no training needed
                            </li>
                            <li>Can outperform more complex parametric (or model-based) methods when decision boundary is complex and difficulty to model
                            </li>
                          </ul>
                        </li>
                        <li>Cons
                          <ul>
                            <li>Can be more computation/storage expensive during query/prediction time if the training data set is large
                            </li>
                            <li>When the dimension is large or some features are irrelevant, nearest neighbors may not be the best predictors (the closest points may not be in the same category as the input data)
                            </li>
                          </ul>
                        </li>
                      </ul>
                    </li>
                    <li>K-mean: <a href="https://www.pyimagesearch.com/2014/05/26/opencv-python-k-means-color-clustering/">k-mean using scikit-learn</a>
                      <ul>
                        <li>Soft K-mean from mixture Gaussian model (see David MacKay's book)
                        </li>
                      </ul>
                    </li>
                    <li>SVM: Support Vector Machine (See <a href='https://www.coursera.org/learn/machine-learning-with-python/home/welcome'>Coursera lectures from IBM</a>):
                      <ul> 
                        <li>Need to answer two questions:
                          <ul>
                            <li>Kernelling: How to transform the data to a higher dim such that the data becomes linearly separable? Linear, Poly, Sigmoid, RBF (Radial Basis Function) (cf RBM: Restricted Boltman Machine)
                            </li>
                            <li>Total least square (w<sup>T</sup>x + b = 0): Maximize the distance between the clusters using only the boundary points, known as support vectors (hence they are memory efficient).
                            </li>
                          </ul>
                        </li>
                        <li>Pros:
                          <ul>
                            <li>Acurate in high dim space
                            </li>
                            <li>Memory efficient
                            </li>
                          </ul>
                        </li>
                        <li>Cons:
                          <ul>
                            <li>Prone to over-fitting if number of features is large compared with the size of the training set
                            </li>
                            <li>Computational intensive for large data set
                            </li>
                            <li>Do not directly provide probability/confidence of the classification for each point
                            </li>
                          </ul>
                        </li>
                        <li>Applications
                          <ul>
                            <li>Image recognition: e.g. classification such as hand-written digit recognition
                            </li>
                            <li>Text category assignment
                            </li>
                            <li>Spam detection
                            </li>
                            <li>Sentiment analysis
                            </li>
                            <li>Gene expression classification
                            </li>
                            <li>Regression, outlier detection and clustering
                            </li>
                          </ul>
                        </li>
                      </ul>
                    </li>
                    <li>Decision tree
                      <ul>
                        <li>Overall algorithm: Start with root that encompass all training data and then recursively (depth or breadth first) apply the following for each node until no nodes can be split:
                          <ul>
                            <li>For each attribute, assess the metric of splitting the node based on it
                            </li>
                            <li>Split the node based on the best attribute selected
                            </li>
                          </ul>
                        </li>
                        <li>Which attribute should be used to split a node?
                          <ul>
                            <li>Selection criterion: Maximize the decrease in entropy, aka information gain, or (entropy of the parent node - average conditional entropy of the children nodes)
                            </li>
                            <li>Intuition behind the above splitting criterion: A child node should be
                              <ul>
                                <li>More predictive or less random: A child node should be better for prediction than the parent node
                                </li>
                                <li>More pure or less impurity: Samples in a child node should be more pure than the parent node
                                </li>
                                <li>Lower entropy: A child node should have lower (conditional) entropy than entropy of the parent node
                                </li>
                              </ul>
                            </li>
                          </ul>
                        </li>
                      </ul>
                    </li>
                    <li>Classification and random forest
                    </li>
                    <li>Boosting and AdaBoosting
                    </li>
                </ul>
            </li>
            <li>Optimization algorithms, cost function
                <ul>
                    <li>Stochastic gradient descend
                    </li>
                    <li><a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">Adam optimization</a>
                    </li>
                    <li>Conjugate gradient method
                    </li>
                </ul>
            </li>
            <li>Tools: Python and TensorFlow
                <ul>
                    <li>A 'hello world': model = keras.Sequential(...), model.compile(..); model.fit(..); model.evaluate(..); model.predict(..)<br>
                        model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]); model.compile(loss='mse', optimizer=''sgd'); xs = np.array(range(7), dtype=float); ys = 5*xs+5; model.fit(xs,ys, epochs=10); print(model.predict([7.0]);
                    </li>
                    <li>MNIST examples: mnist = keras.datasets.mnist or fashion_mnist; (im,lab), (tst_im,tst_lab) = mnist.loaddata(); Flatten(), Dense(..); class myCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): ...
                    </li>
                </ul>
            </li>
            <li>Applications
                <ul>
                    <li>Anormaly detection: supervised or unsupervised?
                    </li>
                    <li>Movie recommender system: linear regression, SVD
                    </li>
                    <li>Computer vision example: photo-OCR, ceiling analysis
                    </li>
                    <li>Robotics
                    </li>
                    <li>Language translation
                    </li>
                    <li><a href="https://pypi.org/project/SpeechRecognition/">Speech recognition</a>
                    </li>
                </ul>
            </li>
        </ul>
        <h4>Grading
        </h4>
             <ul>
                 <li>Homework: 40%</li>
                 <li>Project: 60%</li>
                 <li>Make-up Work: not allowed unless special circumstances</li>
             </ul>
        <h4>Gloassary
        </h4>
             <ul>
                 <li>relu (REctified Linear Unit)
                 </li>
             </ul>
    </body>
</html>
