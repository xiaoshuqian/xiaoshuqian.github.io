<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Probability Distribution Functions
    </title>
  </head>
  <body>
    <h1>Probability Distribution Functions
    </h1>
<h4>References
</h4>
  <ul>
    <li><a href="https://www.csie.ntu.edu.tw/~r97002/temp/Concrete%20Mathematics%202e.pdf">Concrete Math</a> by Graham, Knuth & Pastshnik, 2nd ed.
      <ul>
        <li>Cover both generating functions for combinatorics and probability generating functions
        </li>
      </ul>
    </li>
    <li>Applied Combinatorics by Tucker
      <ul>
        <li>Cover ordinary and exponetial generating functions
        </li>
      </ul>
    </li>
    <li>Mathematical Statistics: Basic Ideas and Selected Topics. Vol 1, by Peter J. Bickel, Kjell A. Doksum; ISBN-13: 978-0132306379; Published by Pearson; 2nd edition (May 4, 2006)
      <ul>
        <li>Cover exponetial familiy
        </li>
      </ul>
    </li>
    <li>Probability, Random Variables and Stochastic Processes by Athanasios Papoulis and S. Unnikrishna Pillai, 4th Ed
      <ul>
        <li>Inside of front and back covers contain
          <ul>
            <li>a list of distributions and their mean, variance and characteristic functions
            </li>
            <li>a list of relationship among different distributions
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="http://www.sze.hu/~harmati/Sztochasztikus%20folyamatok/Probability,_Statistics,_and_Random_Processes_for_Eletrical_Engineerging,_3rd_Ed_-_Leon-Garcia(1).pdf">Probability, Statistics, and Random Processes For Electrical Engineering</a> by Alberto Leon-Garcia, 3rd ed.
      <ul>
        <li>Contains a table of distributions and their mean, variance and characteristic functions
        </li>
      </ul>
    </li>
  </ul>
<h4>Description
</h4>
  <p>
  </p>
<h4>Detailed outline
</h4>
  <ul>
    <li>Tools to study discrete (pmf) and continuous (pdf) Random Variables (RVs)
      <ul>
        <li>Probability Generating Function (PGF = E(z<sup>X</sup>)): esp for non-negative integer RV
          <ul>
            <li>Use G(z) or G(1+t) to find mean and variance or cumulant statistics
            </li>
            <li>Use G(z) to study sum of independent random variables
            </li>
            <li>cf ordinary generating functions (See ref by Tucker or Graham et al 
            </li>
            <li>cf Z-transform for causal sequences
            </li>
          </ul>
        </li>
        <li>Moment Generating Function (MGF = E(e<sup>sX</sup>) = G(z) with z = e<sup>sX</sup>): cf Laplace Transform
        </li>
        <li>Characteristic function: cf Fourier Transform
        </li>
      </ul>
    </li>
    <li>Binomial(n,p)/Poisson(&lambda;T=1)
      <ul>
        <li>Poisson examples: number of chocolate chips in a cookie, number of arrivals in a given period. 
        </li>
        <li>mean and variance: np, np(1-p), &lambda;, &lambda; 
        </li>
      </ul>
    </li>
    <li>Geometric/E(&lambda;), NB(m,p)/&Gamma;(m, &lambda;)
      <ul>
        <li>Distributions of NB(m,p)/&Gamma;(m, &lambda;) can be derived
          <ul>
            <li>using MGF because they are sum of Geometric/E(&lambda;), or
            </li>
            <li>heuristically based on the distributions of B(n,p)/P(&lambda;T)
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Normal, Chi-square, t(n) and F(m,n), non-central versions (see Stevens Kay's detection book)
      <ul>
        <li>Sum(x<sub>i</sub> - x&#772;)<sup>2</sup> is Chi-square distributed with n-1 deg of freedom and is independent of x&#772;
          <ul>
            <li>Key equation: Sum(x<sub>i</sub> - x&#772;)<sup>2</sup> =  Sum(x<sub>i</sub><sup>2</sup>) - n*x&#772;<sup>2</sup>
            </li>
            <li>Two proofs: Use MMG function (Hogg) or Gram-Schmidt (Math551 notes)
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Conjugate prior to exponential family is also an exponential family (ref: Bickel & Docksum). Special cases (Ref: MIT course notes and Bayesian statistics UCSD coursera):
      <ul>
        <li>Beta(a,b) is the conjugate prior distribution of p in Binomial(n,p)
          <ul>
            <li>The posterior is Beta(a+k, b+n-k) if k out of n observations are successes. That is, the effective size (a+b) is increased by n and a is increased by k. 
            </li>
            <li>Mean of Beta(a,b) is a/(a+b) and the variance is ab/[(a+b)<sup>2</sup>(a+b+1)]
            </li>
          </ul>
        </li>
        <li>Dirichlet is the conjugate prior to Multi-nomial
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in Poisson(&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+k, b+1) if k successes are observed in the trial. That is, the effective size (b) is increased by 1 and the number of arrivals a is increased by k. 
            </li>
            <li>Mean of &Gamma;(a,b) is a/b and the variance is a/b<sup>2</sup>
            </li>
          </ul>
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in Exponential E(&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+1, b+t), where t is the inter-arrival time observed. That is, the effective size or the observed time is increased by t and a is increased by 1. 
            </li>
            <li>Mean of E(&lambda;) is 1/&lambda; and the variance is 1/&lambda;<sup>2</sup> (Note: E(&lambda;) is a special case of Gamma: &Gamma;(a,&lambda;) with a = 1)
            </li>
          </ul>
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in &Gamma;(a',&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+a', b+t), where t is the time it takes to get the a'<sup>th</sup> arrival. That is, the effective size or the observed time is increased by t and the number of arrivals a is increased by a'. 
            </li>
          </ul>
        </li>
        <li>N(m, s<sup>2</sup>) is the conjugate prior distribution of &mu; in N(&mu;,&sigma;<sup>2</sup>)
          <ul>
            <li>The posterior is N(M, S<sup>2</sup>), where 1/S<sup>2</sup> = 1/&sigma;<sup>2</sup> + 1/s<sup>2</sup> (Posterior Info = Info gained from data + Prior Info) and M = S<sup>2</sup>(&mu;/&sigma;<sup>2</sup> + x/s<sup>2</sup>) (Posterior mean = weighted avarage of prior mean and mean observed from data. Note if we replace x by ave(x), then we need to replace &sigma;<sup>2</sup> by &sigma;<sup>2</sup>/N where N is the number of independent observations)
            </li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

  </body>
</html>
