<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Probability Distribution Functions
    </title>
  </head>
  <body>
    <h1>Probability Distribution Functions
    </h1>
<h4>References
</h4>
  <ul>
    <li><a href="https://www.ime.usp.br/~abe/lista/pdfz8Fr8IWNI7.pdf">Relationships Among Common Univariate Distributions</a> by Leemis, L. M. (1986)
      <ul>
        <li><a href="https://www.math.wustl.edu/~jmding/math494/dist.pdf">Adapted version</a> by <a href="https://www.math.wustl.edu/~jmding/math494/h494.html">Dimin Ding</a>
        </li>
      </ul>
    </li>
    <li><a href="https://www.csie.ntu.edu.tw/~r97002/temp/Concrete%20Mathematics%202e.pdf">Concrete Math</a> by Graham, Knuth & Pastshnik, 2nd ed.
      <ul>
        <li>Cover both generating functions for combinatorics and probability generating functions
        </li>
      </ul>
    </li>
    <li>Applied Combinatorics by Tucker
      <ul>
        <li>Cover ordinary and exponetial generating functions
        </li>
      </ul>
    </li>
    <li>Mathematical Statistics: Basic Ideas and Selected Topics. Vol 1, by Peter J. Bickel, Kjell A. Doksum; ISBN-13: 978-0132306379; Published by Pearson; 2nd edition (May 4, 2006)
      <ul>
        <li>Cover exponetial familiy
        </li>
      </ul>
    </li>
    <li>Probability, Random Variables and Stochastic Processes by Athanasios Papoulis and S. Unnikrishna Pillai, 4th Ed
      <ul>
        <li>Inside of front and back covers contain
          <ul>
            <li>a list of distributions and their mean, variance and characteristic functions
            </li>
            <li>a list of relationship among different distributions
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="http://www.sze.hu/~harmati/Sztochasztikus%20folyamatok/Probability,_Statistics,_and_Random_Processes_for_Eletrical_Engineerging,_3rd_Ed_-_Leon-Garcia(1).pdf">Probability, Statistics, and Random Processes For Electrical Engineering</a> by Alberto Leon-Garcia, 3rd ed.
      <ul>
        <li>Contains a table of distributions and their mean, variance and characteristic functions
        </li>
      </ul>
    </li>
  </ul>
<h4>Description
</h4>
  <p>
  </p>
<h4>Detailed outline
</h4>
  <ul>
    <li>Tools to study discrete (pmf) and continuous (pdf) Random Variables (RVs)
      <ul>
        <li>Probability Generating Function (PGF = E(z<sup>X</sup>)): esp for non-negative integer RV
          <ul>
            <li>Use G(z) or G(1+t) to find mean and variance or cumulant statistics
            </li>
            <li>Use G(z) to study sum of independent random variables
            </li>
            <li>cf ordinary generating functions (See ref by Tucker or Graham et al 
            </li>
            <li>cf Z-transform for causal sequences
            </li>
          </ul>
        </li>
        <li>Moment Generating Function (MGF = E(e<sup>sX</sup>) = G(z) with z = e<sup>sX</sup>): cf Laplace Transform
        </li>
        <li>Characteristic function: cf Fourier Transform
        </li>
      </ul>
    </li>
    <li>Arrival processes
      <ul>
        <li>Discrete (Num of trials): Num of arrivals out of n trials or number of extra (in addition to m) trials to get m arrivals
          <ul>
            <li>Binomial(n,p) / Bernoulli(p): mean np, variance np(1-p)
            </li>
            <li><a href="https://www.johndcook.com/negative_binomial.pdf">Neg-Binomial(m,p)</a> / Geometric(p): mean m(1-p)/p, variance m(1-p)/p<sup>2</sup>
            </li>
            <li>Multi-nomial(n,p<sub>1</sub>, ...): mean np<sub>1</sub>, ...
            </li>
          </ul>
        </li>
        <li>Continuous (time): Num of arrivals in unit (or T) time or time needed to get m arrivals
          <ul>
            <li>Poisson(&lambda;, T=1): mean &lambda;, &lambda;  (Derived from Binomial)
              <ul>
                <li>Examples: number of chocolate chips in a cookie, number of arrivals in a given period. 
                </li>
              </ul>
            </li>
            <li>&Gamma;(m, &lambda;) / Geometric/E(&lambda;)
            </li>
          </ul>
        <li>Distributions of NB(m,p)/&Gamma;(m, &lambda;) can be derived
          <ul>
            <li>using MGF because they are sum of Geometric/E(&lambda;), or
            </li>
            <li>heuristically based on the distributions of B(n,p)/P(&lambda;T) using a limiting process
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Beta(a, b)/Dirichlet(a<sub>1</sub>, a<sub>2</sub>, ...)
      <ul>
        <li>If X ~ &Gamma;(a, 1),  Y ~ &Gamma;(b, 1), then X/(X+Y) ~ Beta(a,b)
        </li>
      </ul>
    </li>
    <li>Normal, Chi-square, t(n) and F(m,n), non-central versions (see Stevens Kay's detection book)
      <ul>
        <li>Sum(x<sub>i</sub> - x&#772;)<sup>2</sup> is Chi-square distributed with n-1 deg of freedom and is independent of x&#772;
          <ul>
            <li>Key equation: Sum(x<sub>i</sub> - x&#772;)<sup>2</sup> =  Sum(x<sub>i</sub><sup>2</sup>) - n*x&#772;<sup>2</sup>
            </li>
            <li>Two proofs: Use MMG function (Hogg) or Gram-Schmidt (Math551 notes)
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Exponential family: exp(&eta;(&theta;)T(x) - B(&theta;) + h(x))
      <ul>
        <li>Conjugate:  exp(&eta;(&theta;)a - B(&theta;)b) / w(a,b)
        </li>
        <li>X<sub>i</sub> iid~ Exponetial familiy, then their sum is also in an exponential family
        </li>
      </ul>
    </li>
    <li><a href="https://www.johndcook.com/blog/conjugate_prior_diagram/">Conjugate prior</a> to exponential family is also an exponential family (ref: Bickel & Docksum). Special cases (Ref: MIT course notes and Bayesian statistics UCSD coursera) and their <a href="https://en.wikipedia.org/wiki/Jeffreys_prior">Jeffreys priors</a>:
      <ul>
        <li>Beta(a,b) is the conjugate prior distribution of p in Binomial(n,p) and Neg-Binomial(m, p)
          <ul>
            <li>The posterior is Beta(a+k, b+n-k) if k out of n observations are successes. That is, the effective size (a+b) is increased by n and a is increased by k. 
            </li>
            <li>Mean of Beta(a,b) is a/(a+b) and the variance is ab/[(a+b)<sup>2</sup>(a+b+1)]
            </li>
          </ul>
        </li>
        <li>Beta(a,b) is the conjugate prior distribution of p in Neg-Binomial(m, p)
          <ul>
            <li>The posterior is Beta(a+m, b+k)
            </li>
          </ul>
        </li>
        <li>Dirichlet is the conjugate prior to Multi-nomial
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in Poisson(&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+k, b+1) if k successes are observed in the trial. That is, the effective size (b) is increased by 1 and the number of arrivals a is increased by k. 
            </li>
            <li>Mean of &Gamma;(a,b) is a/b and the variance is a/b<sup>2</sup>
            </li>
          </ul>
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in Exponential E(&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+1, b+t), where t is the inter-arrival time observed. That is, the effective size or the observed time is increased by t and a is increased by 1. 
            </li>
            <li>Mean of E(&lambda;) is 1/&lambda; and the variance is 1/&lambda;<sup>2</sup> (Note: E(&lambda;) is a special case of Gamma: &Gamma;(a,&lambda;) with a = 1)
            </li>
          </ul>
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in &Gamma;(a',&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+a', b+t), where t is the time it takes to get the a'<sup>th</sup> arrival. That is, the effective size or the observed time is increased by t and the number of arrivals a is increased by a'. 
            </li>
          </ul>
        </li>
        <li>For fixed &sigma;, &mu; ~ N(m, s<sup>2</sup>) is the conjugate prior distribution of &mu; in N(&mu;,&sigma;<sup>2</sup>)
          <ul>
            <li>The posterior is N(M, S<sup>2</sup>), where 1/S<sup>2</sup> = 1/&sigma;<sup>2</sup> + 1/s<sup>2</sup> (Posterior Info = Info gained from data + Prior Info) and M = S<sup>2</sup>(&mu;/&sigma;<sup>2</sup> + x/s<sup>2</sup>) (Posterior mean = weighted avarage of prior mean and mean observed from data. Note if we replace x by ave(x), then we need to replace &sigma;<sup>2</sup> by &sigma;<sup>2</sup>/N where N is the number of independent observations)
            </li>
          </ul>
        </li>
        <li>&mu; ~ N(m, &sigma;<sup>2</sup>/w), &sigma;<sup>2</sup> ~ &Gamma;<sup>-1</sup>(a,b) are the conjugate prior distributions of &mu; and &sigma;<sup>2</sup> in N(&mu;,&sigma;<sup>2</sup>)
        </li>
      </ul>
    </li>
  </ul>

  </body>
</html>
