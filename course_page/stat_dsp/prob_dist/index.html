<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Probability Distribution Functions
    </title>
  </head>
  <body>
    <h1>Probability Distribution Functions
    </h1>
<h4>References
</h4>
  <ul>
    <li><a href="https://www.ime.usp.br/~abe/lista/pdfz8Fr8IWNI7.pdf">Relationships Among Common Univariate Distributions</a> by Leemis, L. M. (1986)
      <ul>
        <li><a href="https://www.math.wustl.edu/~jmding/math494/dist.pdf">Adapted version</a> by <a href="https://www.math.wustl.edu/~jmding/math494/h494.html">Dimin Ding</a>
        </li>
      </ul>
    </li>
    <li><a href="https://www.csie.ntu.edu.tw/~r97002/temp/Concrete%20Mathematics%202e.pdf">Concrete Math</a> by Graham, Knuth & Pastshnik, 2nd ed.
      <ul>
        <li>Cover both generating functions for combinatorics and probability generating functions
        </li>
      </ul>
    </li>
    <li>Applied Combinatorics by Tucker
      <ul>
        <li>Cover ordinary and exponetial generating functions
        </li>
      </ul>
    </li>
    <li>Mathematical Statistics: Basic Ideas and Selected Topics. Vol 1, by Peter J. Bickel, Kjell A. Doksum; ISBN-13: 978-0132306379; Published by Pearson; 2nd edition (May 4, 2006)
      <ul>
        <li>Cover exponetial familiy
        </li>
      </ul>
    </li>
    <li>Probability, Random Variables and Stochastic Processes by Athanasios Papoulis and S. Unnikrishna Pillai, 4th Ed
      <ul>
        <li>Inside of front and back covers contain
          <ul>
            <li>a list of distributions and their mean, variance and characteristic functions
            </li>
            <li>a list of relationship among different distributions
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="http://www.sze.hu/~harmati/Sztochasztikus%20folyamatok/Probability,_Statistics,_and_Random_Processes_for_Eletrical_Engineerging,_3rd_Ed_-_Leon-Garcia(1).pdf">Probability, Statistics, and Random Processes For Electrical Engineering</a> by Alberto Leon-Garcia, 3rd ed.
      <ul>
        <li>Contains a table of distributions and their mean, variance and characteristic functions
        </li>
      </ul>
    </li>
  </ul>
<h4>Description
</h4>
  <p>
  </p>
<h4>Detailed outline
</h4>
  <ul>
    <li>Tools to study discrete (pmf) and continuous (pdf) Random Variables (RVs)
      <ul>
        <li>Probability Generating Function (PGF = E(z<sup>X</sup>)): esp for non-negative integer RV
          <ul>
            <li>Use G(z) or G(1+t) to find mean and variance or cumulant statistics
            </li>
            <li>Use G(z) to study sum of independent random variables
            </li>
            <li>cf ordinary generating functions (See ref by Tucker or Graham et al 
            </li>
            <li>cf Z-transform for causal sequences
            </li>
          </ul>
        </li>
        <li>Moment Generating Function (MGF = E(e<sup>sX</sup>) = G(z) with z = e<sup>sX</sup>): cf Laplace Transform
        </li>
        <li>Characteristic function: cf Fourier Transform
        </li>
      </ul>
    </li>
    <li>Arrival processes
      <ul>
        <li>Discrete (Num of trials): Num of arrivals out of n trials or number of extra (in addition to m) trials to get m arrivals
          <ul>
            <li>Binomial(n,p) / Bernoulli(p): mean np, variance np(1-p)
            </li>
            <li><a href="https://www.johndcook.com/negative_binomial.pdf">Neg-Binomial(m,p)</a> / Geometric(p):
              <ul>
                <li>var(NB) = mq/p<sup>2</sup> = m * var(geometric_variable), where q = 1-p. 
                  <ul>
                    <li>The variance expression above is applicable regardless of which of the two definitions below is used since the two definitions differ by a constant offset.
                    </li>
                  </ul>
                </li>
                <li>Defn 1: X = # of trials at which the m<sup>th</sup> success occurs.
                  <ul>
                    <li>mean = m/p, since p is the success rate and thus 1/p is the average number of trials needed to get one success.
                    </li>
                    <li>pmf = C(x-1,m-1)p<sup>m</sup>q<sup>x-m</sup>, since the probability of getting (m-1) successes in (x-1) trials is C(x-1,m-1)p<sup>m-1</sup>q<sup>x-m</sup>.
                    </li>
                  </ul>
                </li>
                <li>Defn 2: X = # of failures before the m<sup>th</sup> success.
                  <ul>
                    <li>mean = mq/p, since the two definitions are related by X1 = X2 + m and thus, mean1 = m/p = mean2 + m or mean2 = mq/p.
                    </li>
                    <li>p = m/(mean + m) = #Succ / (#Succ + #Fail)
                    </li>
                    <li>var = mean + mean<sup>2</sup>/m or m = mean/(var - mean<sup>2</sup>). Thus, the var is always bigger than mean (unlike Poison variable whose var = mean). Note that for a Poisson variable with a gamma conjugate prior for its mean, its prior predictive distribution is NB. As the prior distribution becomes more and more concentrated around the mean, the NB approaches the Posson and vice versa.
                    </li>
                    <li>pmf = C(m+x-1,x)p<sup>m</sup>q<sup>x</sup>, since the probability of getting (m-1) successes in (m+x-1) trials is C(m+x-1,m-1)p<sup>m-1</sup>q<sup>x</sup>.
                    </li>
                </li>
                <li>Benefits of defn 2:
                  <ul>
                    <li>The range of X is non-negative integers.
                    </li>
                    <li>No need for m to appear at the bottom of combinatorial coefficient and thus it is easy to generalize m to all non-negative reals.
                    </li>
                  </ul>
                </li>
              </ul>
            </li>
            <li>Multi-nomial(n,p<sub>1</sub>, ...): mean np<sub>1</sub>, ...
            </li>
          </ul>
        </li>
        <li>Continuous (time): Num of arrivals in unit (or T) time or time needed to get m arrivals
          <ul>
            <li>Poisson(&lambda;, T=1): mean &lambda;, &lambda;  (Derived from Binomial)
              <ul>
                <li>Examples: number of chocolate chips in a cookie, number of arrivals in a given period. 
                </li>
              </ul>
            </li>
            <li>&Gamma;(m, &lambda;) / Geometric/E(&lambda;)
            </li>
          </ul>
        <li>Distributions of NB(m,p)/&Gamma;(m, &lambda;) can be derived
          <ul>
            <li>using MGF because they are sum of Geometric/E(&lambda;), or
            </li>
            <li>heuristically based on the distributions of B(n,p)/P(&lambda;T) using a limiting process
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Beta(a, b)/Dirichlet(a<sub>1</sub>, a<sub>2</sub>, ...)
      <ul>
        <li>If X ~ &Gamma;(a, 1),  Y ~ &Gamma;(b, 1), then X/(X+Y) ~ Beta(a,b)
        </li>
      </ul>
    </li>
    <li>Normal, Chi-square, t(n) and F(m,n), non-central versions (see Stevens Kay's detection book)
      <ul>
        <li>Sum(x<sub>i</sub> - x&#772;)<sup>2</sup> is Chi-square distributed with n-1 deg of freedom and is independent of x&#772;
          <ul>
            <li>Key equation: Sum(x<sub>i</sub> - x&#772;)<sup>2</sup> =  Sum(x<sub>i</sub><sup>2</sup>) - n*x&#772;<sup>2</sup>
            </li>
            <li>Two proofs: Use MMG function (Hogg) or Gram-Schmidt (Math551 notes)
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Exponential family: exp(&eta;(&theta;)T(x) - B(&theta;) + h(x))
      <ul>
        <li>Conjugate:  exp(&eta;(&theta;)a - B(&theta;)b) / w(a,b)
        </li>
        <li>X<sub>i</sub> iid~ Exponetial familiy, then their sum is also in an exponential family
        </li>
      </ul>
    </li>
    <li><a href="https://www.johndcook.com/blog/conjugate_prior_diagram/">Conjugate prior</a> to exponential family is also an exponential family (ref: Bickel & Docksum). Special cases (Ref: MIT course notes and Bayesian statistics UCSD coursera) and their <a href="https://en.wikipedia.org/wiki/Jeffreys_prior">Jeffreys priors</a>:
      <ul>
        <li>Beta(a,b) is the conjugate prior distribution of p in Binomial(n,p) and Neg-Binomial(m, p)
          <ul>
            <li>The posterior is Beta(a+k, b+n-k) if k out of n observations are successes. That is, the effective size (a+b) is increased by n and a is increased by k. 
            </li>
            <li>Mean of Beta(a,b) is a/(a+b) and the variance is ab/[(a+b)<sup>2</sup>(a+b+1)]
            </li>
          </ul>
        </li>
        <li>Beta(a,b) is the conjugate prior distribution of p in Neg-Binomial(m, p)
          <ul>
            <li>The posterior is Beta(a+m, b+k)
            </li>
          </ul>
        </li>
        <li>Dirichlet is the conjugate prior to Multi-nomial
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in Poisson(&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+k, b+1) if k successes are observed in the trial. That is, the effective size (b) is increased by 1 and the number of arrivals a is increased by k. 
            </li>
            <li>Mean of &Gamma;(a,b) is a/b and the variance is a/b<sup>2</sup>
            </li>
          </ul>
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in Exponential E(&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+1, b+t), where t is the inter-arrival time observed. That is, the effective size or the observed time is increased by t and a is increased by 1. 
            </li>
            <li>Mean of E(&lambda;) is 1/&lambda; and the variance is 1/&lambda;<sup>2</sup> (Note: E(&lambda;) is a special case of Gamma: &Gamma;(a,&lambda;) with a = 1)
            </li>
          </ul>
        </li>
        <li>&Gamma;(a,b) is the conjugate prior distribution of &lambda; in &Gamma;(a',&lambda;)
          <ul>
            <li>The posterior is &Gamma;(a+a', b+t), where t is the time it takes to get the a'<sup>th</sup> arrival. That is, the effective size or the observed time is increased by t and the number of arrivals a is increased by a'. 
            </li>
          </ul>
        </li>
        <li>For fixed &sigma;, &mu; ~ N(m, s<sup>2</sup>) is the conjugate prior distribution of &mu; in N(&mu;,&sigma;<sup>2</sup>)
          <ul>
            <li>The posterior is N(M, S<sup>2</sup>), where 1/S<sup>2</sup> = 1/&sigma;<sup>2</sup> + 1/s<sup>2</sup> (Posterior Info = Info gained from data + Prior Info) and M = S<sup>2</sup>(&mu;/&sigma;<sup>2</sup> + x/s<sup>2</sup>) (Posterior mean = weighted avarage of prior mean and mean observed from data. Note if we replace x by ave(x), then we need to replace &sigma;<sup>2</sup> by &sigma;<sup>2</sup>/N where N is the number of independent observations)
            </li>
          </ul>
        </li>
        <li>&mu; ~ N(m, &sigma;<sup>2</sup>/w), &sigma;<sup>2</sup> ~ &Gamma;<sup>-1</sup>(a,b) are the conjugate prior distributions of &mu; and &sigma;<sup>2</sup> in N(&mu;,&sigma;<sup>2</sup>)
        </li>
      </ul>
    </li>
  </ul>

  </body>
</html>
