<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>ELEN 334 Statistical Signal Processing
        </title>
    </head>
    <body>
        <h1>ELEN 334 Statistical Signal Processing (2 units)</h1>
        <h2>
            Quarter offered
        </h2>
        <p>
        Summer 2019, 7:10pm -- 8pm.
        </p>
        <h2>
          Prerequisites
        </h2>
        <p> AMTH 211 and ELEN 233 or ELEN 233E.
        </p>
        <h2>
            Course description
        </h2>
        <h4>
            Instructor
        </h4>
        <p>
        Dr. Xiaoshu Qian
        </p>
        <h4>
            Reference books
        </h4>
        <ul>
            <li>Robert Hogg, Joseph McKean and Allen Craig, Introduction to Mathematical Statistics, 8th edition, 2019 Pearson Prentice Hall
              <ul>
                <li><a href="https://www.math.wustl.edu/~jmding/math494/h494.html">slide set 1</a>, <a href="https://math.tntech.edu/e-stat/6270/">slide set 2</a>
                </li>
              </ul>
            </li>
            <li><a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/class-slides/">MIT probability and statistics course materials</a>
            <li> Jaynes, E.T. (2003), Probability Theory: The Logic of Science, Cambridge, UK, Cambridge University Press.
            <a href="http://www.med.mcgill.ca/epidemiology/hanley/bios601/GaussianModel/JaynesProbabilityTheory.pdf">book_pdf</a>
            </li>
        </ul>
        <h4>
            Description
        </h4>
        <p>
        Introduction to statistical signal processing concepts. Random variables, random vectors, and random processes. Second-moment analysis, estimation of first and second moments of a random process. Linear transformations; the matched filter. Spectral factorization, innovation representations of random processes. The orthogonality principle. Linear predictive filtering; linear prediction and AR models. Levinson algorithm. Burg algorithm.
        </p>
        <h4> Detailed outline </h4>
        <ul>
            <li>Probability
                <ul>
                    <li>Axioms
                    </li>
                    <li>Interpretation: random vs uncertain
                    </li>
                    <li>Conditional probability and independence
                    </li>
                    <li>Law of total probability (tree diagram)
                    </li>
                    <li>Bayes theorem
                    </li>
                </ul>
            </li>
            <li>Random variables, their distributions and generating functions
            </li>
            <li>Law of large numbers and central limit theorem
            </li>
            <li>Statistical problem <a href="https://www.math.wustl.edu/~jmding/math494/h494.html">slide set 1</a>, <a href="https://math.tntech.edu/e-stat/6270/">slide set 2</a>
                <ul>
                  <li>Random experiments: experimental design or observational studies
                  </li>
                  <li>Data
                  </li>
                  <li>Analysis: Descriptive/Explanatory analysis
                  </li>
                  <li>Inference: Assume a distribution or model for the population and use data samples to infer properties of the distribution (or features of population) that generates the data
                    <ul>
                      <li>Models
                        <ul>
                          <li>Non-parametric model
                          </li>
                          <li>Parametric model
                          </li>
                          <li>Semiparametric model
                          </li>
                        </ul>
                      </li>
                      <li>Inferences
                        <ul>
                          <li>Point estimates and their variances
                          </li>
                          <li>Confidence intervals (cf Credible interval in Bayesian approach)
                          </li>
                          <li>Hypothesis tests
                          </li>
                          <li>Duality between confidence interval and hypothesis test
                          </li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li>Examples: population, data, model
                    <ul>
                      <li>Quality control: Hypergeometric model for number of defectives out of a size-n sample
                      </li>
                      <li>Measurement problem: Gaussian model
                      </li>
                      <li>Treatment effect of drug A and B
                      </li>
                    </ul>
                  </li>
                </ul>
            </li>
            <li>Point estimates and their variances
            </li>
            <li>Confidence intervals (cf Credible interval in Bayesian approach)
            </li>
            <li>Hypothesis tests
              <ul>
                <li>Z-test
                </li>
                <li>t-test
                </li>
                <li>Goodness of fit (Chi-square) test
                </li>
                <li>ANOVA (F-test)
                </li>
                <li>Linear regression
                </li>
              </ul>
            </li>
            <li>Duality between confidence interval and hypothesis test
            </li>
            <li>Statistical inference: models and algorithms
                <ul>
                  <li><a href="https://medium.com/app-affairs/9-applications-of-machine-learning-from-day-to-day-life-112a47a429d0">Applications from day-to-day life</a>
                  </li>
                  <li><a href="https://medium.com/app-affairs/9-applications-of-machine-learning-from-day-to-day-life-112a47a429d0">Popular algorithms in ML</a>
                    <ul>
                        <li>Linear regression
                        </li>
                        <li>Logistic regression
                        </li>
                        <li>Linear desriminator analysis
                        </li>
                        <li>Naive Bayes
                        </li>
                        <li>Neural network
                        </li>
                        <li>Nearest neighbor
                        </li>
                        <li>K-mean
                        </li>
                        <li>Vector support
                        </li>
                        <li>PCA
                        </li>
                        <li>Classification and decision tree, random forest
                        </li>
                    </ul>
                  </li>
                </ul>
            </li>
            <li>Stochastic process, correlation function and PSD
            </li>
            <li>Filter, interpolation and prediction
              <ul>
                <li>Wiener filter
                </li>
              </ul>
            </li>
        </ul>
        <h4>Grading
        </h4>
             <ul>
                 <li>Homework: 20%</li>
                 <li>Midterm: 30%</li>
                 <li>Final: 50%</li>
                 <li>Make-up work: not allowed unless special circumstances</li>
             </ul>
    </body>
</html>
