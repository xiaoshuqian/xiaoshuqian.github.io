<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>ELEN 334 Statistical Signal Processing
        </title>
    </head>
    <body>
        <h1>ELEN 334 Statistical Signal Processing (2 units)</h1>
        <h2>
            Quarter offered
        </h2>
        <p>
        Summer 2019, 7:10pm -- 8pm.
        </p>
        <h2>
          Prerequisites
        </h2>
        <p> AMTH 211 and ELEN 233 or ELEN 233E.
        </p>
        <h2>
            Course description
        </h2>
        <h4>
            Instructor
        </h4>
        <p>
        Dr. Xiaoshu Qian
        </p>
        <h4>
            Reference books
        </h4>
        <ul>
            <li><a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/class-slides/">MIT probability and statistics course materials</a>
            <li> Jaynes, E.T. (2003), Probability Theory: The Logic of Science, Cambridge, UK, Cambridge University Press.
            <a href="http://www.med.mcgill.ca/epidemiology/hanley/bios601/GaussianModel/JaynesProbabilityTheory.pdf">book_pdf</a>
            </li>
        </ul>
        <h4>
            Description
        </h4>
        <p>
        Introduction to statistical signal processing concepts. Random variables, random vectors, and random processes. Second-moment analysis, estimation of first and second moments of a random process. Linear transformations; the matched filter. Spectral factorization, innovation representations of random processes. The orthogonality principle. Linear predictive filtering; linear prediction and AR models. Levinson algorithm. Burg algorithm.
        </p>
        <h4> Detailed outline </h4>
        <ul>
            <li>Probability
                <ul>
                    <li>Axioms
                    </li>
                    <li>Interpretation: random vs uncertain
                    </li>
                    <li>Conditional probability and independence
                    </li>
                    <li>Law of total probability (tree diagram)
                    </li>
                    <li>Bayes theorem
                    </li>
                </ul>
            </li>
            <li>Random variables
            </li>
            <li>Statistics
                <ul>
                    <li><a href="https://medium.com/app-affairs/9-applications-of-machine-learning-from-day-to-day-life-112a47a429d0">Applications from day-to-day life</a>
                    </li>
                    <li><a href="https://medium.com/app-affairs/9-applications-of-machine-learning-from-day-to-day-life-112a47a429d0">Popular algorithms in ML</a>
                    </li>
                    <ul>
                        <li>Linear regression
                        </li>
                        <li>Logistic regression
                        </li>
                        <li>Linear desriminator analysis
                        </li>
                        <li>Naive Bayes
                        </li>
                        <li>Neural network
                        </li>
                        <li>Nearest neighbor
                        </li>
                        <li>K-mean
                        </li>
                        <li>Vector support
                        </li>
                        <li>PCA
                        </li>
                        <li>Classification and decision tree, random forest
                        </li>
                    </ul>
                    </ul>
                </ul>
            </li>
        </ul>
        <h4>Grading
        </h4>
             <ul>
                 <li>Homework: 20%</li>
                 <li>Midterm: 30%</li>
                 <li>Final: 50%</li>
                 <li>Make-up work: not allowed unless special circumstances</li>
             </ul>
    </body>
</html>
